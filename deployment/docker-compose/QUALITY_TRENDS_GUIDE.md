# ğŸ“Š Tracking Answer Quality Trends in Opik

## Overview
Opik provides comprehensive tools to track and visualize answer quality trends over time, helping you monitor model performance and identify degradation or improvements.

## ğŸ¯ What You Can Track

### 1. **Automated Quality Metrics**
Track these metrics automatically on every evaluation run:

- **Overall Quality Score** (0-1) - Composite quality measure
- **Relevance Score** (0-1) - How well the answer addresses the question
- **Accuracy Score** (0-1) - Factual correctness vs ground truth
- **Conciseness Score** (0-1) - Appropriate length and clarity
- **Similarity Score** (0-1) - Closeness to expected answer

### 2. **User Feedback Metrics**
Collect and trend real user feedback:

- â­ Star ratings (1-5)
- ğŸ‘ğŸ‘ Thumbs up/down
- ğŸ’¬ Qualitative feedback
- ğŸ·ï¸ Issue categories (accuracy, relevance, tone, etc.)

### 3. **Performance Metrics**
Monitor operational aspects:

- â±ï¸ Response latency
- ğŸ”„ Retry rates
- âŒ Error rates
- ğŸ’° Token usage / costs

## ğŸ“ˆ How to View Trends in Opik UI

### Method 1: Experiments Dashboard

1. **Open Opik UI**: http://localhost:5173
2. **Navigate to Experiments** tab
3. **View Options**:
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Experiments                            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  âœ“ quality-tracking-20251225-0900       â”‚
   â”‚  âœ“ quality-tracking-20251224-0900       â”‚
   â”‚  âœ“ quality-tracking-20251223-0900       â”‚
   â”‚                                         â”‚
   â”‚  [Compare Selected]  [Export]           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

4. **Compare Multiple Experiments**:
   - Select 2+ experiments
   - Click "Compare"
   - View side-by-side metric comparisons

### Method 2: Time Series View

1. Navigate to **Analytics** section
2. Select metrics to visualize
3. Choose date range
4. View trends:
   ```
   Quality Score Over Time
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1.0 â”‚                    â•±â”€â•²   â”‚
   â”‚ 0.8 â”‚                â•±â”€â”€â•¯   â•²  â”‚
   â”‚ 0.6 â”‚            â•±â”€â”€â•¯         â”‚
   â”‚ 0.4 â”‚        â•±â”€â”€â•¯             â”‚
   â”‚ 0.2 â”‚    â•±â”€â”€â•¯                 â”‚
   â”‚ 0.0 â”‚â”€â”€â”€â•¯                     â”‚
   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Dec 20  21   22   23   24   25
   ```

### Method 3: Category-Based Analysis

Track quality by question category:
```
Technical Questions:    0.92 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
Support Questions:      0.85 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
General Questions:      0.78 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
Product Questions:      0.91 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
```

## ğŸ”§ Implementation Example

### Basic Setup
```python
from opik.evaluation.metrics import base_metric

class QualityScore(base_metric.BaseMetric):
    def score(self, output: str, expected: str = None):
        # Your quality assessment logic
        return quality_value  # 0.0 to 1.0

# Run daily and track over time
results = evaluate(
    experiment_name=f"quality-{date}",
    task=your_task,
    scoring_metrics=[QualityScore()],
)
```

### With User Feedback
```python
from opik import Opik

client = Opik()

# When user rates an answer
client.log_traces_feedback_scores(
    project_name="my-project",
    scores=[{
        "trace_id": trace_id,
        "name": "user_satisfaction",
        "value": rating / 5.0,  # 1-5 stars normalized
        "reason": user_comment,
    }]
)
```

## ğŸ“Š Dashboard Examples

### Quality Scorecard Dashboard
Create a custom dashboard showing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Quality Metrics - Last 7 Days                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Overall Quality:        0.89  â†‘ +0.03             â”‚
â”‚  Relevance:             0.92  â†‘ +0.05             â”‚
â”‚  Accuracy:              0.87  â†“ -0.02  âš ï¸          â”‚
â”‚  Conciseness:           0.85  â†’ +0.00             â”‚
â”‚  User Satisfaction:     4.2/5  â†‘ +0.1             â”‚
â”‚                                                     â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                     â”‚
â”‚  Alerts:                                           â”‚
â”‚  âš ï¸  Accuracy dropped 2% - investigate tech Q's    â”‚
â”‚  âœ…  Relevance improved - model update working     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trend Comparison Dashboard
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Week-over-Week Comparison                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Metric            This Week   Last Week   Change  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Quality           0.89        0.86        â†‘ +3%   â”‚
â”‚  Relevance         0.92        0.87        â†‘ +5%   â”‚
â”‚  Accuracy          0.87        0.89        â†“ -2%   â”‚
â”‚  Response Time     245ms       312ms       â†‘ +21%  â”‚
â”‚  User Rating       4.2â­       4.1â­        â†‘       â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”” Setting Up Alerts

### Option 1: Using Opik API
```python
def check_quality_degradation():
    # Get recent experiments
    recent_scores = get_recent_scores(days=7)
    
    if recent_scores['quality'] < 0.80:  # Threshold
        send_alert(
            title="Quality Score Below Threshold",
            message=f"Current: {recent_scores['quality']:.2f}",
            severity="warning"
        )
```

### Option 2: Using Monitoring Tools
Integrate with tools like:
- **Grafana** - Real-time dashboards
- **Datadog** - APM monitoring
- **PagerDuty** - Alert management
- **Slack** - Team notifications

### Option 3: Email Notifications
```python
def send_quality_report(frequency='daily'):
    results = get_latest_evaluation()
    
    if results.quality_score < previous_score * 0.95:  # 5% drop
        send_email(
            to="team@company.com",
            subject="âš ï¸ Quality Alert: Performance Degradation",
            body=generate_report(results)
        )
```

## ğŸ“ˆ Advanced Analytics

### 1. Statistical Analysis
```python
import pandas as pd

# Export Opik data to DataFrame
experiments_df = pd.DataFrame([
    {
        'date': exp.date,
        'quality': exp.scores['quality'],
        'relevance': exp.scores['relevance'],
    }
    for exp in get_all_experiments()
])

# Calculate trends
experiments_df['quality_ma'] = experiments_df['quality'].rolling(7).mean()
experiments_df['quality_std'] = experiments_df['quality'].rolling(7).std()

# Detect anomalies
anomalies = experiments_df[
    (experiments_df['quality'] < experiments_df['quality_ma'] - 2 * experiments_df['quality_std'])
]
```

### 2. Correlation Analysis
```python
# Find correlations between metrics
correlation_matrix = experiments_df[['quality', 'relevance', 'conciseness']].corr()

# Example: If relevance drops, quality often drops too
if correlation_matrix.loc['relevance', 'quality'] > 0.7:
    print("High correlation: Focus on improving relevance")
```

### 3. Category Performance
```python
# Track quality by question category
category_trends = {
    'technical': experiments_df[experiments_df['category'] == 'technical']['quality'].mean(),
    'support': experiments_df[experiments_df['category'] == 'support']['quality'].mean(),
    'general': experiments_df[experiments_df['category'] == 'general']['quality'].mean(),
}

# Identify weak areas
weakest = min(category_trends, key=category_trends.get)
print(f"Focus improvement efforts on: {weakest}")
```

## ğŸ¯ Best Practices

### 1. **Consistent Evaluation Schedule**
```bash
# Run at the same time daily
0 9 * * * /path/to/quality_tracking_example.py
```

### 2. **Version Everything**
Track model versions, prompts, and configs:
```python
experiment_config={
    "model_version": "gpt-4-2024-01",
    "prompt_version": "v2.3",
    "temperature": 0.7,
    "git_commit": get_git_commit(),
}
```

### 3. **Use Multiple Metrics**
Don't rely on a single metric:
- Automated scores (objective)
- User feedback (subjective)
- Business KPIs (e.g., resolution rate)

### 4. **Set Appropriate Thresholds**
```python
thresholds = {
    'quality': 0.80,      # Minimum acceptable
    'relevance': 0.85,
    'user_rating': 4.0,
}
```

### 5. **Regular Reviews**
- **Daily**: Automated checks
- **Weekly**: Team review of trends
- **Monthly**: Deep dive analysis

## ğŸ” Troubleshooting Quality Issues

When you notice quality degradation:

1. **Check Recent Changes**
   - Model version updates?
   - Prompt changes?
   - New data sources?

2. **Drill Down by Category**
   - Which question types are affected?
   - Is it specific to certain topics?

3. **Review Individual Cases**
   - Look at low-scoring examples
   - Identify patterns in failures

4. **Compare with Baseline**
   - Use Opik's comparison view
   - Look at before/after metrics

## ğŸ“š Resources

- **Opik Documentation**: https://github.com/comet-ml/opik
- **Evaluation Guide**: See `evaluation_script.py`
- **Custom Metrics**: See `quality_tracking_example.py`
- **API Reference**: http://localhost:5173/api/docs

## ğŸš€ Getting Started

1. Run the quality tracking script:
   ```bash
   python quality_tracking_example.py
   ```

2. View results in Opik UI:
   ```bash
   open http://localhost:5173
   ```

3. Set up daily runs:
   ```bash
   # Add to crontab
   0 9 * * * /path/to/quality_tracking_example.py
   ```

4. Create your dashboard and start tracking! ğŸ“Š
